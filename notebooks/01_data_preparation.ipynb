{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 1: Data Preparation\n",
        "\n",
        "## Purpose\n",
        "Ingest, merge, and clean all raw data to produce a single, unified, English-only dataset.\n",
        "\n",
        "## Objectives\n",
        "1. Load and merge all CSV files from comments and Metadata directories\n",
        "2. Handle duplicate comment_ids and video_ids\n",
        "3. Perform language detection and filter to English comments only\n",
        "4. Optimize data types for memory efficiency\n",
        "5. Save cleaned dataset for downstream analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from pathlib import Path\n",
        "from langdetect import detect, LangDetectException\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import gc\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Merge Data Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_file_to_topic(file_path):\n",
        "    \"\"\"\n",
        "    Extract topic from filename using pattern matching.\n",
        "    Handles all naming variations observed in raw data files.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Full path to CSV file\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    str or None\n",
        "        One of the 5 valid topics, or None if no match found\n",
        "        \n",
        "    Notes:\n",
        "    ------\n",
        "    Order matters! Check most specific patterns first to avoid false matches.\n",
        "    Pattern matching is based on ACTUAL filenames in raw/comments/ and raw/Metadata/\n",
        "    \"\"\"\n",
        "    file_name = os.path.basename(file_path)\n",
        "    \n",
        "    # Venus phosphine - check full pattern first, then short version\n",
        "    if 'Venus_phosphine' in file_name:\n",
        "        return 'Venus phosphine'\n",
        "    elif 'Venus' in file_name:  # Handles Venus_comments files\n",
        "        return 'Venus phosphine'\n",
        "    \n",
        "    # 3I/ATLAS - files use \"3IATLAS\" (no slash, no spaces)\n",
        "    elif '3IATLAS' in file_name:\n",
        "        return '3I/ATLAS'\n",
        "    \n",
        "    # K2-18b - files use \"K2-18b\" (with hyphens)\n",
        "    elif 'K2-18b' in file_name:\n",
        "        return 'K2-18b'\n",
        "    \n",
        "    # Oumuamua - straightforward naming\n",
        "    elif 'Oumuamua' in file_name:\n",
        "        return 'Oumuamua'\n",
        "    \n",
        "    # Tabby's Star - files use \"Tabbys\" or \"Tabbys_Star\" (no apostrophe)\n",
        "    elif 'Tabbys_Star' in file_name or 'Tabbys' in file_name:\n",
        "        return \"Tabby's Star\"\n",
        "    \n",
        "    # No match found - will trigger warning\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def load_csv_files(directory_path, file_pattern=\"*.csv\"):\n",
        "    \"\"\"\n",
        "    Load and concatenate CSV files from a directory.\n",
        "    Adds 'file_source_topic' column to each DataFrame based on filename.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    directory_path : str\n",
        "        Path to directory containing CSV files\n",
        "    file_pattern : str\n",
        "        Glob pattern for CSV files (default: \"*.csv\")\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple : (combined_df, num_files)\n",
        "        - combined_df: Concatenated DataFrame with file_source_topic column\n",
        "        - num_files: Number of files successfully loaded\n",
        "        \n",
        "    Notes:\n",
        "    ------\n",
        "    The file_source_topic column serves as the GROUND TRUTH for topic assignment.\n",
        "    It prevents corruption by extracting topics directly from filenames during load.\n",
        "    \"\"\"\n",
        "    csv_files = glob.glob(os.path.join(directory_path, file_pattern))\n",
        "    print(f\"Found {len(csv_files)} CSV files in {directory_path}\")\n",
        "    \n",
        "    if len(csv_files) == 0:\n",
        "        raise ValueError(f\"No CSV files found in {directory_path}\")\n",
        "    \n",
        "    dataframes = []\n",
        "    topic_counts = {}\n",
        "    unknown_files = []\n",
        "    \n",
        "    for file_path in tqdm(csv_files, desc=\"Loading CSV files\"):\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, low_memory=False)\n",
        "            file_name = os.path.basename(file_path)\n",
        "            \n",
        "            # Extract topic from filename (GROUND TRUTH!)\n",
        "            topic = map_file_to_topic(file_path)\n",
        "            \n",
        "            if topic:\n",
        "                df['file_source_topic'] = topic\n",
        "                topic_counts[topic] = topic_counts.get(topic, 0) + len(df)\n",
        "                print(f\"‚úì {file_name}: {len(df):,} rows ‚Üí {topic}\")\n",
        "            else:\n",
        "                # Flag unknown files but still load them\n",
        "                print(f\"‚ùå WARNING: Could not determine topic for {file_name}\")\n",
        "                df['file_source_topic'] = 'UNKNOWN'\n",
        "                unknown_files.append(file_name)\n",
        "            \n",
        "            dataframes.append(df)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading {file_path}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if len(dataframes) == 0:\n",
        "        raise ValueError(\"No dataframes were successfully loaded\")\n",
        "    \n",
        "    # Concatenate all dataframes\n",
        "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"LOADING SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Successfully loaded {len(dataframes)} files\")\n",
        "    print(f\"Total rows: {len(combined_df):,}\")\n",
        "    \n",
        "    print(f\"\\nTopic breakdown:\")\n",
        "    for topic, count in sorted(topic_counts.items()):\n",
        "        print(f\"  {topic}: {count:,} rows\")\n",
        "    \n",
        "    if unknown_files:\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: {len(unknown_files)} file(s) with UNKNOWN topic:\")\n",
        "        for f in unknown_files:\n",
        "            print(f\"    - {f}\")\n",
        "        print(\"These files need manual review!\")\n",
        "    \n",
        "    print(f\"{'='*60}\\n\")\n",
        "    \n",
        "    return combined_df, len(csv_files)\n",
        "\n",
        "# Load comments data\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING COMMENTS DATA\")\n",
        "print(\"=\" * 60)\n",
        "comments_df, num_comment_files = load_csv_files(\"../data/raw/comments/\")\n",
        "\n",
        "# Load metadata\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING METADATA\")\n",
        "print(\"=\" * 60)\n",
        "metadata_df, num_metadata_files = load_csv_files(\"../data/raw/Metadata/\")\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Comment files: {num_comment_files}\")\n",
        "print(f\"  Metadata files: {num_metadata_files}\")\n",
        "print(f\"  Total comments: {len(comments_df):,}\")\n",
        "print(f\"  Total metadata records: {len(metadata_df):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation: Verify file-to-topic mapping worked correctly\n",
        "print(\"=\" * 60)\n",
        "print(\"FILE-TO-TOPIC MAPPING VALIDATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Validate comments\n",
        "if 'file_source_topic' in comments_df.columns:\n",
        "    print(\"\\n‚úÖ Comments files - Topic distribution:\")\n",
        "    topic_dist = comments_df['file_source_topic'].value_counts()\n",
        "    print(topic_dist)\n",
        "    \n",
        "    unknown_count = (comments_df['file_source_topic'] == 'UNKNOWN').sum()\n",
        "    if unknown_count > 0:\n",
        "        print(f\"\\n‚ùå WARNING: {unknown_count:,} comment rows with UNKNOWN topic!\")\n",
        "        print(\"Some files didn't match expected naming patterns.\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ All {len(comments_df):,} comment rows have valid topics\")\n",
        "else:\n",
        "    raise ValueError(\"‚ùå CRITICAL: file_source_topic column missing from comments_df!\")\n",
        "\n",
        "# Validate metadata\n",
        "if 'file_source_topic' in metadata_df.columns:\n",
        "    print(\"\\n‚úÖ Metadata files - Topic distribution:\")\n",
        "    print(metadata_df['file_source_topic'].value_counts())\n",
        "    \n",
        "    unknown_count_meta = (metadata_df['file_source_topic'] == 'UNKNOWN').sum()\n",
        "    if unknown_count_meta > 0:\n",
        "        print(f\"\\n‚ùå WARNING: {unknown_count_meta:,} metadata rows with UNKNOWN topic!\")\n",
        "else:\n",
        "    raise ValueError(\"‚ùå CRITICAL: file_source_topic column missing from metadata_df!\")\n",
        "\n",
        "# Verify expected file counts\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FILE COUNT VERIFICATION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Expected: 7 comment files (2√ó3I/ATLAS, 2√óK2-18b, 1√óVenus, 1√óOumuamua, 1√óTabby's)\")\n",
        "print(f\"Expected: 5 metadata files (1 per topic)\")\n",
        "print(f\"\\nActual comment files loaded: {num_comment_files}\")\n",
        "print(f\"Actual metadata files loaded: {num_metadata_files}\")\n",
        "\n",
        "if num_comment_files != 7:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Expected 7 comment files, but loaded {num_comment_files}\")\n",
        "    print(\"Check if any files are missing from raw/comments/\")\n",
        "\n",
        "if num_metadata_files != 5:\n",
        "    print(f\"‚ö†Ô∏è  WARNING: Expected 5 metadata files, but loaded {num_metadata_files}\")\n",
        "    print(\"Check if any files are missing from raw/Metadata/\")\n",
        "\n",
        "print(\"\\n‚úÖ Validation complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Handle Duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate comment_ids before merging\n",
        "print(\"=\" * 60)\n",
        "print(\"DUPLICATE DETECTION - COMMENTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "duplicate_comments_before = comments_df.duplicated(subset=['comment_id']).sum()\n",
        "print(f\"Duplicate comment_ids before cleaning: {duplicate_comments_before:,}\")\n",
        "\n",
        "if duplicate_comments_before > 0:\n",
        "    print(f\"Percentage of duplicates: {(duplicate_comments_before/len(comments_df)*100):.2f}%\")\n",
        "    # Remove duplicates, keeping first occurrence\n",
        "    comments_df = comments_df.drop_duplicates(subset=['comment_id'], keep='first')\n",
        "    print(f\"After removing duplicates: {len(comments_df):,} comments\")\n",
        "    print(f\"Removed {duplicate_comments_before:,} duplicate comments\")\n",
        "else:\n",
        "    print(\"No duplicate comment_ids found\")\n",
        "\n",
        "# Check for duplicate video_ids in metadata\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DUPLICATE DETECTION - METADATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "duplicate_videos_before = metadata_df.duplicated(subset=['video_id']).sum()\n",
        "print(f\"Duplicate video_ids before cleaning: {duplicate_videos_before:,}\")\n",
        "\n",
        "if duplicate_videos_before > 0:\n",
        "    print(f\"Percentage of duplicates: {(duplicate_videos_before/len(metadata_df)*100):.2f}%\")\n",
        "    # Remove duplicates, keeping first occurrence\n",
        "    metadata_df = metadata_df.drop_duplicates(subset=['video_id'], keep='first')\n",
        "    print(f\"After removing duplicates: {len(metadata_df):,} metadata records\")\n",
        "    print(f\"Removed {duplicate_videos_before:,} duplicate metadata records\")\n",
        "else:\n",
        "    print(\"No duplicate video_ids found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Merge Comments and Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge comments with metadata on video_id\n",
        "print(\"=\" * 60)\n",
        "print(\"MERGING COMMENTS AND METADATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nComments shape: {comments_df.shape}\")\n",
        "print(f\"Metadata shape: {metadata_df.shape}\")\n",
        "\n",
        "# Identify columns to merge from metadata\n",
        "# EXCLUDE: video_id (join key), file_source_topic (we use comment's version), search_query (unreliable)\n",
        "metadata_cols_to_merge = [\n",
        "    col for col in metadata_df.columns \n",
        "    if col not in comments_df.columns \n",
        "    and col not in ['video_id', 'file_source_topic', 'search_query']\n",
        "]\n",
        "\n",
        "print(f\"\\nColumns to add from metadata: {metadata_cols_to_merge}\")\n",
        "\n",
        "# Perform LEFT JOIN to keep all comments\n",
        "# We use comments_df as the base because it has the reliable file_source_topic\n",
        "merged_df = comments_df.merge(\n",
        "    metadata_df[['video_id'] + metadata_cols_to_merge],\n",
        "    on='video_id',\n",
        "    how='left',\n",
        "    suffixes=('', '_from_metadata')\n",
        ")\n",
        "\n",
        "print(f\"\\nMerge complete. Shape: {merged_df.shape}\")\n",
        "\n",
        "# Check for any new duplicates introduced by merge\n",
        "duplicate_comments_after = merged_df.duplicated(subset=['comment_id']).sum()\n",
        "print(f\"Duplicate comment_ids after merge: {duplicate_comments_after:,}\")\n",
        "\n",
        "if duplicate_comments_after > 0:\n",
        "    print(\"‚ö†Ô∏è  WARNING: Duplicates introduced during merge!\")\n",
        "    merged_df = merged_df.drop_duplicates(subset=['comment_id'], keep='first')\n",
        "    print(f\"After removing duplicates: {len(merged_df):,} comments\")\n",
        "\n",
        "# === CORRUPTION FIX: Nuclear Option - Delete and Recreate ===\n",
        "# Delete old corrupted column completely, then create fresh one\n",
        "# This ensures no metadata baggage from the corrupted column survives\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"APPLYING CORRUPTION FIX - DELETE AND RECREATE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "VALID_TOPICS = {'3I/ATLAS', 'Oumuamua', 'K2-18b', \"Tabby's Star\", 'Venus phosphine'}\n",
        "\n",
        "if 'file_source_topic' in merged_df.columns:\n",
        "    # STEP 1: Show what we're removing\n",
        "    if 'search_query' in merged_df.columns:\n",
        "        old_unique = merged_df['search_query'].nunique()\n",
        "        old_nan = merged_df['search_query'].isna().sum()\n",
        "        old_invalid = (~merged_df['search_query'].isin(VALID_TOPICS)).sum()\n",
        "        \n",
        "        print(f\"\\nOld search_query column stats:\")\n",
        "        print(f\"  Unique values: {old_unique}\")\n",
        "        print(f\"  NaN values: {old_nan:,}\")\n",
        "        print(f\"  Invalid values: {old_invalid:,}\")\n",
        "        \n",
        "        # COMPLETELY DELETE the old corrupted column\n",
        "        print(f\"\\nüóëÔ∏è  Deleting old corrupted search_query column...\")\n",
        "        merged_df = merged_df.drop(columns=['search_query'])\n",
        "        print(f\"‚úì Old search_query column DELETED\")\n",
        "    else:\n",
        "        print(f\"\\n‚ÑπÔ∏è  No existing search_query column found (good)\")\n",
        "    \n",
        "    # STEP 2: Create brand NEW search_query from filename-based topics\n",
        "    print(f\"\\nüÜï Creating fresh search_query from file_source_topic...\")\n",
        "    merged_df['search_query'] = merged_df['file_source_topic'].copy()\n",
        "    print(f\"‚úì New search_query column CREATED from filenames\")\n",
        "    \n",
        "    # STEP 3: Verify the new column\n",
        "    new_unique = merged_df['search_query'].nunique()\n",
        "    new_nan = merged_df['search_query'].isna().sum()\n",
        "    new_invalid = (~merged_df['search_query'].isin(VALID_TOPICS)).sum()\n",
        "    \n",
        "    print(f\"\\nNew search_query column stats:\")\n",
        "    print(f\"  Unique values: {new_unique}\")\n",
        "    print(f\"  NaN values: {new_nan:,}\")\n",
        "    print(f\"  Invalid values: {new_invalid:,}\")\n",
        "    print(f\"  Data type: {merged_df['search_query'].dtype}\")\n",
        "    \n",
        "    # STEP 4: Clean up temporary column\n",
        "    merged_df = merged_df.drop(columns=['file_source_topic'])\n",
        "    print(f\"\\n‚úì Cleaned up temporary file_source_topic column\")\n",
        "    \n",
        "    # STEP 5: Final validation\n",
        "    if new_invalid == 0 and new_unique == 5 and new_nan == 0:\n",
        "        print(f\"\\n‚úÖ CORRUPTION FIX SUCCESSFUL!\")\n",
        "        print(f\"   All {len(merged_df):,} rows have valid topics\")\n",
        "        print(f\"   Topics: {sorted(merged_df['search_query'].unique().tolist())}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå CORRUPTION FIX FAILED!\")\n",
        "        print(f\"   Invalid rows: {new_invalid:,}\")\n",
        "        print(f\"   NaN rows: {new_nan:,}\")\n",
        "        print(f\"   Unique topics: {new_unique} (expected 5)\")\n",
        "        raise ValueError(\"Corruption fix validation failed\")\n",
        "else:\n",
        "    raise ValueError(\"‚ùå CRITICAL: file_source_topic column not found!\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Additional validation (redundant but safe)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ADDITIONAL VALIDATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "VALID_TOPICS = {'3I/ATLAS', 'Oumuamua', 'K2-18b', \"Tabby's Star\", 'Venus phosphine'}\n",
        "\n",
        "# Check for NaN values\n",
        "nan_count = merged_df['search_query'].isna().sum()\n",
        "print(f\"NaN values in search_query: {nan_count:,}\")\n",
        "\n",
        "# Check for invalid values\n",
        "invalid_mask = ~merged_df['search_query'].isin(VALID_TOPICS)\n",
        "invalid_count = invalid_mask.sum()\n",
        "print(f\"Invalid topic values: {invalid_count:,}\")\n",
        "\n",
        "# Show final distribution\n",
        "print(f\"\\nFinal topic distribution:\")\n",
        "topic_dist_final = merged_df['search_query'].value_counts()\n",
        "print(topic_dist_final)\n",
        "\n",
        "# Verify we have exactly 5 topics\n",
        "if len(topic_dist_final) != 5:\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNING: Expected exactly 5 topics, found {len(topic_dist_final)}\")\n",
        "    raise ValueError(\"Expected exactly 5 topics after corruption fix\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Confirmed: Exactly 5 unique topics as expected\")\n",
        "\n",
        "if nan_count > 0 or invalid_count > 0:\n",
        "    print(f\"\\n‚ùå VALIDATION FAILED!\")\n",
        "    print(f\"   {nan_count:,} NaN values remain\")\n",
        "    print(f\"   {invalid_count:,} invalid values remain\")\n",
        "    raise ValueError(\"Corruption fix failed - data still contains invalid topics\")\n",
        "\n",
        "print(f\"\\n‚úÖ VALIDATION PASSED: All topics are valid!\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "\n",
        "# Store in df for next steps\n",
        "df = merged_df.copy()\n",
        "print(f\"\\nFinal merged dataset:\")\n",
        "print(f\"  Rows: {len(df):,}\")\n",
        "print(f\"  Columns: {len(df.columns)}\")\n",
        "print(f\"  Topics: {df['search_query'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Language Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_language_safe(text):\n",
        "    \"\"\"\n",
        "    Detect language with robust error handling.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text : str\n",
        "        Text to detect language for\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    str : Language code or 'unknown'\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or str(text).strip() == '':\n",
        "        return 'unknown'\n",
        "    \n",
        "    try:\n",
        "        text_str = str(text)\n",
        "        # Skip very short texts (langdetect needs sufficient text)\n",
        "        if len(text_str.strip()) < 3:\n",
        "            return 'unknown'\n",
        "        \n",
        "        language = detect(text_str)\n",
        "        return language\n",
        "    except LangDetectException:\n",
        "        return 'unknown'\n",
        "    except Exception as e:\n",
        "        return 'unknown'\n",
        "\n",
        "# Enable progress bar for pandas apply\n",
        "tqdm.pandas(desc=\"Detecting languages\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LANGUAGE DETECTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Use comment_text_original for language detection (as per specification)\n",
        "print(\"Detecting languages using 'comment_text_original' column...\")\n",
        "df['language'] = df['comment_text_original'].progress_apply(detect_language_safe)\n",
        "\n",
        "print(f\"\\nLanguage detection complete!\")\n",
        "print(f\"Total comments processed: {len(df):,}\")\n",
        "\n",
        "# Display language distribution\n",
        "language_counts = df['language'].value_counts()\n",
        "language_percentages = (language_counts / len(df) * 100).round(2)\n",
        "\n",
        "language_distribution = pd.DataFrame({\n",
        "    'language': language_counts.index,\n",
        "    'count': language_counts.values,\n",
        "    'percentage': language_percentages.values\n",
        "})\n",
        "\n",
        "print(\"\\nTop 10 languages:\")\n",
        "print(language_distribution.head(10).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize language distribution\n",
        "print(\"=\" * 60)\n",
        "print(\"LANGUAGE DISTRIBUTION VISUALIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get top 10 languages\n",
        "top_10_languages = language_distribution.head(10)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "bars = ax.barh(top_10_languages['language'], top_10_languages['count'],\n",
        "               color='steelblue', edgecolor='black', linewidth=0.5)\n",
        "ax.set_xlabel('Number of Comments', fontweight='bold', fontsize=12)\n",
        "ax.set_ylabel('Language', fontweight='bold', fontsize=12)\n",
        "ax.set_title('Top 10 Languages in YouTube Comments', \n",
        "            fontweight='bold', fontsize=14, pad=20)\n",
        "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "\n",
        "# Add value labels\n",
        "for i, (idx, row) in enumerate(top_10_languages.iterrows()):\n",
        "    ax.text(row['count'] + max(top_10_languages['count']) * 0.01, i,\n",
        "            f\"{int(row['count']):,} ({row['percentage']:.1f}%)\",\n",
        "            va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../outputs/figures/language_distribution.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"‚úì Language distribution plot saved to outputs/figures/language_distribution.png\")\n",
        "plt.close()\n",
        "\n",
        "# Save full language distribution table\n",
        "language_distribution.to_csv('../outputs/tables/language_distribution.csv', index=False)\n",
        "print(\"‚úì Language distribution table saved to outputs/tables/language_distribution.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Filter to English Comments Only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"FILTERING TO ENGLISH COMMENTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "total_comments = len(df)\n",
        "english_comments = df[df['language'] == 'en']\n",
        "non_english_comments = total_comments - len(english_comments)\n",
        "non_english_percentage = (non_english_comments / total_comments * 100)\n",
        "\n",
        "print(f\"Total comments: {total_comments:,}\")\n",
        "print(f\"English comments: {len(english_comments):,}\")\n",
        "print(f\"Non-English comments: {non_english_comments:,} ({non_english_percentage:.2f}%)\")\n",
        "\n",
        "# Filter to English only\n",
        "df_english = english_comments.copy()\n",
        "\n",
        "print(f\"\\nFiltered dataset: {len(df_english):,} English comments\")\n",
        "print(f\"Removed: {non_english_comments:,} non-English comments ({non_english_percentage:.2f}%)\")\n",
        "\n",
        "# Justification for English-only filtering\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"JUSTIFICATION FOR ENGLISH-ONLY FILTERING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "English-only filtering is justified for the following reasons:\n",
        "\n",
        "1. Sentiment analysis models (TextBlob, VADER, Transformer) are primarily \n",
        "   trained on English text and perform best on English content.\n",
        "\n",
        "2. Mixing languages would introduce noise and reduce model accuracy, as \n",
        "   sentiment cues vary significantly across languages.\n",
        "\n",
        "3. For a focused research question on astrobiology topics, maintaining \n",
        "   language consistency ensures comparability across topics.\n",
        "\n",
        "4. The English subset represents a substantial portion of the dataset, \n",
        "   ensuring sufficient statistical power for analysis.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Data Type Optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"DATA TYPE OPTIMIZATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Convert engagement counts to int32\n",
        "engagement_cols = ['like_count', 'reply_count']\n",
        "for col in engagement_cols:\n",
        "    if col in df_english.columns:\n",
        "        df_english[col] = pd.to_numeric(df_english[col], errors='coerce').fillna(0).astype('int32')\n",
        "        print(f\"‚úì Converted {col} to int32\")\n",
        "\n",
        "# Keep video_view_count as int64 (may exceed int32 range)\n",
        "if 'video_view_count' in df_english.columns:\n",
        "    df_english['video_view_count'] = pd.to_numeric(df_english['video_view_count'], errors='coerce').fillna(0).astype('int64')\n",
        "    print(f\"‚úì Converted video_view_count to int64\")\n",
        "\n",
        "# Convert categorical columns to category type for memory efficiency\n",
        "# CRITICAL FIX: search_query should NOT be converted to category\n",
        "# because it causes issues with unused categories from before filtering.\n",
        "# When pandas converts to category after filtering, it can preserve\n",
        "# unused category values from the original DataFrame, causing corruption.\n",
        "categorical_cols = ['channel_title', 'comment_type']  # Removed 'search_query' from this list\n",
        "for col in categorical_cols:\n",
        "    if col in df_english.columns:\n",
        "        df_english[col] = df_english[col].astype('category')\n",
        "        print(f\"‚úì Converted {col} to category\")\n",
        "\n",
        "# Keep search_query as string/object type to prevent categorical corruption\n",
        "# This ensures clean data when saving to CSV and reloading\n",
        "if 'search_query' in df_english.columns:\n",
        "    # Ensure it's a clean string type (remove any categorical metadata)\n",
        "    df_english['search_query'] = df_english['search_query'].astype('str')\n",
        "    print(f\"‚úì Kept search_query as string type (prevents categorical corruption)\")\n",
        "\n",
        "# Verify search_query has exactly 5 topics\n",
        "if 'search_query' in df_english.columns:\n",
        "    unique_topics = df_english['search_query'].nunique()\n",
        "    print(f\"\\n‚úì search_query validation:\")\n",
        "    print(f\"  Unique topics: {unique_topics}\")\n",
        "    print(f\"  Expected: 5 topics\")\n",
        "    \n",
        "    if unique_topics != 5:\n",
        "        print(f\"‚ö†Ô∏è  WARNING: Expected 5 topics, found {unique_topics}\")\n",
        "        print(f\"  Topics found: {df_english['search_query'].unique().tolist()}\")\n",
        "    else:\n",
        "        print(f\"  ‚úÖ Confirmed: Exactly 5 topics as expected\")\n",
        "        print(f\"  Topics: {sorted(df_english['search_query'].unique().tolist())}\")\n",
        "\n",
        "# Display memory usage\n",
        "memory_before = df_english.memory_usage(deep=True).sum() / 1024**2\n",
        "print(f\"\\nMemory usage: {memory_before:.2f} MB\")\n",
        "\n",
        "# Display data types\n",
        "print(\"\\nData types:\")\n",
        "print(df_english.dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Save Cleaned Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"SAVING CLEANED DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save English-only dataset\n",
        "output_path = '../data/processed/01_comments_english.csv'\n",
        "df_english.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"‚úì Saved cleaned dataset to: {output_path}\")\n",
        "print(f\"  Rows: {len(df_english):,}\")\n",
        "print(f\"  Columns: {len(df_english.columns)}\")\n",
        "print(f\"  Memory: {df_english.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DATASET SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total comments (English): {len(df_english):,}\")\n",
        "print(f\"Unique videos: {df_english['video_id'].nunique():,}\")\n",
        "print(f\"Unique topics: {df_english['search_query'].nunique() if 'search_query' in df_english.columns else 'N/A'}\")\n",
        "print(f\"Date range: {df_english['published_at'].min() if 'published_at' in df_english.columns else 'N/A'} to {df_english['published_at'].max() if 'published_at' in df_english.columns else 'N/A'}\")\n",
        "\n",
        "# Clean up memory\n",
        "del comments_df, metadata_df, merged_df, df\n",
        "gc.collect()\n",
        "print(\"\\n‚úì Memory cleaned up\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"NOTEBOOK 1 COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Next step: Run Notebook 2 (Exploratory Data Analysis)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
