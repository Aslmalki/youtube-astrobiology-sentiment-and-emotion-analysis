{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 3: Preprocessing and Feature Engineering\n",
        "\n",
        "## Purpose\n",
        "Handle all data transformations with differential preprocessing for each sentiment model.\n",
        "\n",
        "## Objectives\n",
        "1. Implement four-track preprocessing strategy (TextBlob, VADER, Transformer, Raw)\n",
        "2. Extract engagement features (question marks, exclamations, caps ratio, emoji count)\n",
        "3. Validate preprocessing approaches\n",
        "4. Save preprocessed dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import contractions\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully\")\n",
        "print(\"NLTK data downloaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load cleaned English dataset\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df = pd.read_csv('../data/processed/01_comments_english.csv')\n",
        "print(f\"Loaded dataset: {len(df):,} comments, {len(df.columns)} columns\")\n",
        "\n",
        "# Ensure comment_text_original exists\n",
        "if 'comment_text_original' not in df.columns:\n",
        "    raise ValueError(\"comment_text_original column not found in dataset\")\n",
        "\n",
        "# Create text_raw column (no preprocessing)\n",
        "df['text_raw'] = df['comment_text_original'].astype(str)\n",
        "print(f\"âœ“ Created text_raw column\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Four-Track Preprocessing Strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_for_textblob(text):\n",
        "    \"\"\"\n",
        "    Heavy preprocessing for TextBlob.\n",
        "    Removes punctuation, emojis, stopwords, and applies lemmatization.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or str(text).strip() == '':\n",
        "        return \"\"\n",
        "    \n",
        "    text = str(text)\n",
        "    \n",
        "    # 1. Expand contractions\n",
        "    try:\n",
        "        text = contractions.fix(text)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # 2. Lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # 3. Remove URLs, mentions, hashtags\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    \n",
        "    # 4. Remove special characters and emojis\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    \n",
        "    # 5. Tokenize\n",
        "    try:\n",
        "        tokens = word_tokenize(text)\n",
        "    except:\n",
        "        tokens = text.split()\n",
        "    \n",
        "    # 6. Remove stopwords\n",
        "    try:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # 7. Lemmatization\n",
        "    try:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # 8. Join and clean\n",
        "    text = ' '.join(tokens).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def preprocess_for_vader(text):\n",
        "    \"\"\"\n",
        "    Minimal preprocessing for VADER.\n",
        "    \n",
        "    CRITICAL: Keep punctuation, caps, emojis - VADER needs these!\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or str(text).strip() == '':\n",
        "        return \"\"\n",
        "    \n",
        "    text = str(text)\n",
        "    \n",
        "    # ONLY remove URLs and mentions\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    \n",
        "    # Clean whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    # DO NOT remove: punctuation, caps, emojis, hashtags\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "def preprocess_for_transformer(text):\n",
        "    \"\"\"\n",
        "    Light preprocessing for Transformers.\n",
        "    Keep structure, remove only noise.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or str(text).strip() == '':\n",
        "        return \"\"\n",
        "    \n",
        "    text = str(text)\n",
        "    \n",
        "    # Remove URLs and mentions\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    \n",
        "    # Clean whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    # Keep: punctuation, caps, emojis, structure\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "print(\"Preprocessing functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply all preprocessing tracks\n",
        "print(\"=\" * 60)\n",
        "print(\"CREATING FOUR PREPROCESSING TRACKS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Enable progress bar\n",
        "tqdm.pandas(desc=\"Processing\")\n",
        "\n",
        "# Track A: Heavy preprocessing for TextBlob\n",
        "print(\"\\n1. Creating text_textblob (heavy preprocessing)...\")\n",
        "df['text_textblob'] = df['text_raw'].progress_apply(preprocess_for_textblob)\n",
        "\n",
        "# Track B: Minimal preprocessing for VADER\n",
        "print(\"\\n2. Creating text_vader (minimal preprocessing - KEEPS punctuation, caps, emojis)...\")\n",
        "df['text_vader'] = df['text_raw'].progress_apply(preprocess_for_vader)\n",
        "\n",
        "# Track C: Light preprocessing for Transformers\n",
        "print(\"\\n3. Creating text_transformer (light preprocessing)...\")\n",
        "df['text_transformer'] = df['text_raw'].progress_apply(preprocess_for_transformer)\n",
        "\n",
        "# Track D: text_raw already exists (no preprocessing)\n",
        "print(\"\\n4. text_raw preserved for engagement features\")\n",
        "\n",
        "print(\"\\nâœ“ All four preprocessing tracks created\")\n",
        "print(f\"  text_raw: {len(df[df['text_raw'].str.strip() != ''])}, non-empty texts\")\n",
        "print(f\"  text_textblob: {len(df[df['text_textblob'].str.strip() != ''])}, non-empty texts\")\n",
        "print(f\"  text_vader: {len(df[df['text_vader'].str.strip() != ''])}, non-empty texts\")\n",
        "print(f\"  text_transformer: {len(df[df['text_transformer'].str.strip() != ''])}, non-empty texts\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Preprocessing Validation Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test case for validation\n",
        "print(\"=\" * 60)\n",
        "print(\"PREPROCESSING VALIDATION TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_text = \"I can't believe how AMAZING this discovery is!!! ðŸš€\"\n",
        "\n",
        "text_textblob = preprocess_for_textblob(test_text)\n",
        "text_vader = preprocess_for_vader(test_text)\n",
        "text_transformer = preprocess_for_transformer(test_text)\n",
        "\n",
        "print(f\"\\nOriginal:    {test_text}\")\n",
        "print(f\"TextBlob:    {text_textblob}\")\n",
        "print(f\"VADER:       {text_vader}\")\n",
        "print(f\"Transformer: {text_transformer}\")\n",
        "\n",
        "# Assertions\n",
        "try:\n",
        "    assert '!!!' not in text_textblob, \"TextBlob: Punctuation should be removed\"\n",
        "    assert 'ðŸš€' not in text_textblob, \"TextBlob: Emojis should be removed\"\n",
        "    assert text_textblob.islower(), \"TextBlob: Should be lowercase\"\n",
        "    \n",
        "    assert '!!!' in text_vader, \"VADER: Punctuation MUST be preserved!\"\n",
        "    assert 'ðŸš€' in text_vader, \"VADER: Emojis MUST be preserved!\"\n",
        "    assert 'AMAZING' in text_vader, \"VADER: Capitalization MUST be preserved!\"\n",
        "    \n",
        "    assert '!!!' in text_transformer, \"Transformer: Punctuation MUST be preserved!\"\n",
        "    \n",
        "    print(\"\\nâœ“âœ“âœ“ All preprocessing validation tests PASSED! âœ“âœ“âœ“\")\n",
        "except AssertionError as e:\n",
        "    print(f\"\\nâœ—âœ—âœ— VALIDATION FAILED: {e} âœ—âœ—âœ—\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXTRACTING ENGAGEMENT FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Vectorized features (FAST)\n",
        "df['question_count'] = df['text_raw'].str.count('\\?').fillna(0).astype('int32')\n",
        "df['exclamation_count'] = df['text_raw'].str.count('!').fillna(0).astype('int32')\n",
        "df['text_length'] = df['text_raw'].str.len().fillna(0).astype('int32')\n",
        "df['word_count'] = df['text_raw'].str.split().str.len().fillna(0).astype('int32')\n",
        "\n",
        "print(\"âœ“ Created vectorized features (question_count, exclamation_count, text_length, word_count)\")\n",
        "\n",
        "# Caps ratio (requires apply)\n",
        "def calculate_caps_ratio(text):\n",
        "    if pd.isna(text) or len(str(text)) == 0:\n",
        "        return 0.0\n",
        "    text_str = str(text)\n",
        "    return sum(1 for c in text_str if c.isupper()) / len(text_str)\n",
        "\n",
        "tqdm.pandas(desc=\"Calculating caps ratio\")\n",
        "df['caps_ratio'] = df['text_raw'].progress_apply(calculate_caps_ratio).astype('float32')\n",
        "print(\"âœ“ Created caps_ratio\")\n",
        "\n",
        "# Emoji count (requires apply)\n",
        "def count_emojis(text):\n",
        "    if pd.isna(text):\n",
        "        return 0\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return len(emoji_pattern.findall(str(text)))\n",
        "\n",
        "tqdm.pandas(desc=\"Counting emojis\")\n",
        "df['emoji_count'] = df['text_raw'].progress_apply(count_emojis).astype('int32')\n",
        "print(\"âœ“ Created emoji_count\")\n",
        "\n",
        "print(\"\\nâœ“ All engagement features extracted\")\n",
        "print(f\"  Question marks: {df['question_count'].sum():,} total\")\n",
        "print(f\"  Exclamation marks: {df['exclamation_count'].sum():,} total\")\n",
        "print(f\"  Emojis: {df['emoji_count'].sum():,} total\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Save Preprocessed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save preprocessed data\n",
        "output_path = '../data/processed/02_preprocessed_data.csv'\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SAVING PREPROCESSED DATA\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"âœ“ Preprocessed data saved: {output_path}\")\n",
        "print(f\"  Columns: {len(df.columns)}\")\n",
        "print(f\"  Rows: {len(df):,}\")\n",
        "print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Display column names\n",
        "print(\"\\nPreprocessing columns:\")\n",
        "preprocessing_cols = [col for col in df.columns if col.startswith('text_')]\n",
        "for col in preprocessing_cols:\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\nEngagement feature columns:\")\n",
        "engagement_cols = ['question_count', 'exclamation_count', 'text_length', 'word_count', 'caps_ratio', 'emoji_count']\n",
        "for col in engagement_cols:\n",
        "    if col in df.columns:\n",
        "        print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"NOTEBOOK 3 COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Next step: Run Notebook 4 (Sentiment Modeling and Validation)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
