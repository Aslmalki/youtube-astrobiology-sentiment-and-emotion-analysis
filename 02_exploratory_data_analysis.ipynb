{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 2: Exploratory Data Analysis\n",
        "\n",
        "## Purpose\n",
        "Perform comprehensive exploratory data analysis on the cleaned English-only dataset.\n",
        "\n",
        "## Objectives\n",
        "1. Create text features (length, word count)\n",
        "2. Analyze distributions of numerical variables\n",
        "3. Detect outliers using Tukey's method\n",
        "4. Analyze correlations between variables\n",
        "5. Examine topic distribution\n",
        "6. Generate summary statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from scipy import stats\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Data and Create Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load cleaned English dataset\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df = pd.read_csv('../data/processed/01_comments_english.csv')\n",
        "print(f\"Loaded dataset: {len(df):,} comments, {len(df.columns)} columns\")\n",
        "\n",
        "# Create text features with proper type conversion\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CREATING TEXT FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df['text_length'] = df['comment_text_original'].str.len().fillna(0).astype('int32')\n",
        "df['word_count'] = df['comment_text_original'].str.split().str.len().fillna(0).astype('int32')\n",
        "\n",
        "print(f\"✓ Created text_length (character count)\")\n",
        "print(f\"✓ Created word_count (word count)\")\n",
        "\n",
        "# CRITICAL: Convert all numeric columns to proper numeric types\n",
        "numeric_cols = ['like_count', 'reply_count', 'text_length', 'word_count']\n",
        "if 'video_view_count' in df.columns:\n",
        "    numeric_cols.append('video_view_count')\n",
        "\n",
        "print(\"\\nConverting numeric columns to proper types...\")\n",
        "for col in numeric_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "    if col != 'video_view_count':  # Keep video_view_count as int64\n",
        "        df[col] = df[col].astype('int32')\n",
        "    print(f\"✓ Converted {col} to {'int64' if col == 'video_view_count' else 'int32'}\")\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_distribution_publication_quality(data, column, title, xlabel):\n",
        "    \"\"\"\n",
        "    Create publication-quality distribution plots WITHOUT log scales.\n",
        "    \n",
        "    Uses two approaches:\n",
        "    1. Full distribution histogram (normal scale)\n",
        "    2. Zoomed histogram focusing on main data range (where 95% of data lies)\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pd.DataFrame\n",
        "        Input data\n",
        "    column : str\n",
        "        Column to plot\n",
        "    title : str\n",
        "        Plot title\n",
        "    xlabel : str\n",
        "        X-axis label\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Convert to numeric and filter out NaN\n",
        "    plot_data = pd.to_numeric(data[column], errors='coerce').dropna()\n",
        "    \n",
        "    if len(plot_data) == 0:\n",
        "        print(f\"Warning: No numeric data to plot for {column}\")\n",
        "        return\n",
        "    \n",
        "    # Calculate percentiles for zoomed view\n",
        "    p95 = plot_data.quantile(0.95)\n",
        "    p99 = plot_data.quantile(0.99)\n",
        "    \n",
        "    # LEFT: Full distribution (normal scale, all data)\n",
        "    axes[0].hist(plot_data, bins=50, edgecolor='black', linewidth=0.5,\n",
        "                 color='steelblue', alpha=0.8)\n",
        "    axes[0].set_xlabel(xlabel, fontweight='bold', fontsize=12)\n",
        "    axes[0].set_ylabel('Frequency', fontweight='bold', fontsize=12)\n",
        "    axes[0].set_title(f'{title} - Full Distribution', fontweight='bold', fontsize=13)\n",
        "    axes[0].grid(alpha=0.3, linestyle='--', linewidth=0.5)\n",
        "    \n",
        "    # Add summary text box\n",
        "    summary_text = f'Mean: {plot_data.mean():.1f}\\\\nMedian: {plot_data.median():.1f}\\\\n95th %ile: {p95:.1f}'\n",
        "    axes[0].text(0.7, 0.9, summary_text, transform=axes[0].transAxes,\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
        "                verticalalignment='top', fontsize=9)\n",
        "    \n",
        "    # RIGHT: Zoomed to main range (95th percentile)\n",
        "    main_range_data = plot_data[plot_data <= p95]\n",
        "    axes[1].hist(main_range_data, bins=50, edgecolor='black', linewidth=0.5,\n",
        "                 color='steelblue', alpha=0.8)\n",
        "    axes[1].set_xlabel(xlabel, fontweight='bold', fontsize=12)\n",
        "    axes[1].set_ylabel('Frequency', fontweight='bold', fontsize=12)\n",
        "    axes[1].set_title(f'{title} - Main Range (0-95th %ile)', fontweight='bold', fontsize=13)\n",
        "    axes[1].grid(alpha=0.3, linestyle='--', linewidth=0.5)\n",
        "    \n",
        "    # Add info text\n",
        "    pct_in_range = len(main_range_data) / len(plot_data) * 100\n",
        "    info_text = f'{pct_in_range:.1f}% of data\\\\nRange: 0-{p95:.0f}'\n",
        "    axes[1].text(0.7, 0.9, info_text, transform=axes[1].transAxes,\n",
        "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
        "                verticalalignment='top', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    filename = f\"../outputs/figures/distribution_{column.lower()}.png\"\n",
        "    plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')\n",
        "    print(f\"✓ Saved: {filename}\")\n",
        "    plt.close()\n",
        "\n",
        "# Apply to all numerical variables\n",
        "print(\"=\" * 60)\n",
        "print(\"Distribution Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "plot_distribution_publication_quality(df, 'like_count', 'Like Count Distribution', 'Number of Likes')\n",
        "plot_distribution_publication_quality(df, 'reply_count', 'Reply Count Distribution', 'Number of Replies')\n",
        "plot_distribution_publication_quality(df, 'text_length', 'Text Length Distribution', 'Character Count')\n",
        "plot_distribution_publication_quality(df, 'word_count', 'Word Count Distribution', 'Number of Words')\n",
        "\n",
        "if 'video_view_count' in df.columns:\n",
        "    plot_distribution_publication_quality(df, 'video_view_count', 'Video View Count Distribution', 'Number of Views')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_outliers_tukey(data, column):\n",
        "    \"\"\"\n",
        "    Detect outliers using Tukey's method (Q1 - 1.5*IQR, Q3 + 1.5*IQR).\n",
        "    \n",
        "    CRITICAL FIX: Properly converts data to numeric BEFORE any calculations.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pd.DataFrame\n",
        "        Input DataFrame\n",
        "    column : str\n",
        "        Column name to analyze\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Outlier statistics\n",
        "    \"\"\"\n",
        "    # CRITICAL: Convert to numeric first and drop NaN\n",
        "    numeric_data = pd.to_numeric(data[column], errors='coerce').dropna()\n",
        "    \n",
        "    if len(numeric_data) == 0:\n",
        "        return {\n",
        "            'column': column,\n",
        "            'count': 0,\n",
        "            'outliers_count': 0,\n",
        "            'outliers_pct': 0.0,\n",
        "            'Q1': 0,\n",
        "            'Q3': 0,\n",
        "            'IQR': 0,\n",
        "            'lower_bound': 0,\n",
        "            'upper_bound': 0\n",
        "        }\n",
        "    \n",
        "    # Calculate quartiles and IQR\n",
        "    Q1 = numeric_data.quantile(0.25)\n",
        "    Q3 = numeric_data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    \n",
        "    # Calculate outlier boundaries\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    \n",
        "    # Identify outliers\n",
        "    outliers = (numeric_data < lower_bound) | (numeric_data > upper_bound)\n",
        "    outliers_count = outliers.sum()\n",
        "    outliers_pct = (outliers_count / len(numeric_data)) * 100\n",
        "    \n",
        "    return {\n",
        "        'column': column,\n",
        "        'count': len(numeric_data),\n",
        "        'outliers_count': outliers_count,\n",
        "        'outliers_pct': outliers_pct,\n",
        "        'Q1': Q1,\n",
        "        'Q3': Q3,\n",
        "        'IQR': IQR,\n",
        "        'lower_bound': lower_bound,\n",
        "        'upper_bound': upper_bound\n",
        "    }\n",
        "\n",
        "# Apply outlier detection to all numerical columns\n",
        "print(\"=\" * 60)\n",
        "print(\"Outlier Detection (Tukey's Method)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "outlier_results = []\n",
        "for col in numeric_cols:\n",
        "    result = detect_outliers_tukey(df, col)\n",
        "    outlier_results.append(result)\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Total values: {result['count']:,}\")\n",
        "    print(f\"  Outliers: {result['outliers_count']:,} ({result['outliers_pct']:.2f}%)\")\n",
        "    print(f\"  Boundaries: [{result['lower_bound']:.2f}, {result['upper_bound']:.2f}]\")\n",
        "\n",
        "# Save outlier analysis\n",
        "outlier_df = pd.DataFrame(outlier_results)\n",
        "outlier_df.to_csv('../outputs/tables/outlier_analysis.csv', index=False)\n",
        "print(\"\\n✓ Outlier analysis saved to outputs/tables/outlier_analysis.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create boxplots for outlier visualization\n",
        "print(\"=\" * 60)\n",
        "print(\"Boxplot Visualization\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "plot_cols = ['like_count', 'reply_count', 'text_length', 'word_count']\n",
        "\n",
        "for idx, col in enumerate(plot_cols):\n",
        "    # Convert to numeric\n",
        "    plot_data = pd.to_numeric(df[col], errors='coerce').dropna()\n",
        "    \n",
        "    if len(plot_data) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Create boxplot\n",
        "    axes[idx].boxplot([plot_data], vert=True, patch_artist=True,\n",
        "                     boxprops=dict(facecolor='steelblue', alpha=0.7),\n",
        "                     medianprops=dict(color='red', linewidth=2),\n",
        "                     flierprops=dict(marker='o', markerfacecolor='red', \n",
        "                                    markersize=3, alpha=0.5))\n",
        "    \n",
        "    axes[idx].set_ylabel(col.replace('_', ' ').title(), fontweight='bold', fontsize=12)\n",
        "    axes[idx].set_title(f'{col.replace(\"_\", \" \").title()}', \n",
        "                       fontweight='bold', fontsize=13)\n",
        "    axes[idx].grid(alpha=0.3, linestyle='--', axis='y')\n",
        "    \n",
        "    # Use log scale BUT with readable labels (not 10^x)\n",
        "    axes[idx].set_yscale('log')\n",
        "    \n",
        "    # CRITICAL FIX: Set readable tick labels instead of 10^x notation\n",
        "    # Calculate appropriate tick positions\n",
        "    y_min = max(plot_data.min(), 1)  # Avoid log(0)\n",
        "    y_max = plot_data.max()\n",
        "    \n",
        "    # Create tick positions at powers of 10\n",
        "    tick_positions = []\n",
        "    power = 0\n",
        "    while 10**power < y_max:\n",
        "        if 10**power >= y_min:\n",
        "            tick_positions.append(10**power)\n",
        "        power += 1\n",
        "    \n",
        "    if len(tick_positions) > 0:\n",
        "        axes[idx].set_yticks(tick_positions)\n",
        "        # Format labels as regular numbers with commas\n",
        "        tick_labels = [f'{int(val):,}' if val < 1000 else f'{int(val/1000):,}K' if val < 1000000 else f'{int(val/1000000):,}M' \n",
        "                      for val in tick_positions]\n",
        "        axes[idx].set_yticklabels(tick_labels)\n",
        "    \n",
        "    # Add note about log scale\n",
        "    axes[idx].text(0.5, 0.02, 'Log scale', transform=axes[idx].transAxes,\n",
        "                  fontsize=8, style='italic', ha='center',\n",
        "                  bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../outputs/figures/outlier_boxplots.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"✓ Boxplots saved to outputs/figures/outlier_boxplots.png\")\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select numerical columns for correlation\n",
        "numerical_cols_for_corr = ['like_count', 'reply_count', 'text_length', 'word_count']\n",
        "if 'video_view_count' in df.columns:\n",
        "    numerical_cols_for_corr.append('video_view_count')\n",
        "\n",
        "# CRITICAL: Ensure all columns are numeric\n",
        "corr_data = df[numerical_cols_for_corr].copy()\n",
        "for col in numerical_cols_for_corr:\n",
        "    corr_data[col] = pd.to_numeric(corr_data[col], errors='coerce')\n",
        "\n",
        "# Drop rows with any NaN values\n",
        "corr_data = corr_data.dropna()\n",
        "\n",
        "# Calculate correlation matrices\n",
        "corr_spearman = corr_data.corr(method='spearman')\n",
        "corr_pearson = corr_data.corr(method='pearson')\n",
        "\n",
        "# Create heatmap visualization\n",
        "print(\"=\" * 60)\n",
        "print(\"Correlation Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Spearman correlation\n",
        "sns.heatmap(corr_spearman, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, ax=axes[0],\n",
        "            vmin=-1, vmax=1)\n",
        "axes[0].set_title('Spearman Correlation Matrix\\n(For Skewed Distributions)', \n",
        "                 fontweight='bold', fontsize=14, pad=15)\n",
        "\n",
        "# Pearson correlation\n",
        "sns.heatmap(corr_pearson, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, ax=axes[1],\n",
        "            vmin=-1, vmax=1)\n",
        "axes[1].set_title('Pearson Correlation Matrix\\n(Linear Relationships)', \n",
        "                 fontweight='bold', fontsize=14, pad=15)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../outputs/figures/correlation_matrix.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "print(\"✓ Correlation matrices saved\")\n",
        "plt.close()\n",
        "\n",
        "# Save correlation tables\n",
        "corr_spearman.to_csv('../outputs/tables/correlation_spearman.csv')\n",
        "corr_pearson.to_csv('../outputs/tables/correlation_pearson.csv')\n",
        "print(\"✓ Correlation tables saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Topic Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze topic distribution\n",
        "print(\"=\" * 60)\n",
        "print(\"Topic Distribution Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if 'search_query' in df.columns:\n",
        "    topic_counts = df['search_query'].value_counts()\n",
        "    topic_percentages = (topic_counts / len(df) * 100).round(2)\n",
        "    \n",
        "    topic_distribution = pd.DataFrame({\n",
        "        'topic': topic_counts.index,\n",
        "        'count': topic_counts.values,\n",
        "        'percentage': topic_percentages.values\n",
        "    })\n",
        "    \n",
        "    print(topic_distribution.to_string(index=False))\n",
        "    \n",
        "    # Save table\n",
        "    topic_distribution.to_csv('../outputs/tables/topic_distribution.csv', index=False)\n",
        "    print(\"\\n✓ Topic distribution table saved\")\n",
        "    \n",
        "    # Create horizontal bar chart\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    bars = ax.barh(topic_distribution['topic'], topic_distribution['count'],\n",
        "                   color='steelblue', edgecolor='black', linewidth=0.5)\n",
        "    ax.set_xlabel('Number of Comments', fontweight='bold', fontsize=12)\n",
        "    ax.set_ylabel('Topic', fontweight='bold', fontsize=12)\n",
        "    ax.set_title('Comment Distribution Across Astrobiology Topics', \n",
        "                fontweight='bold', fontsize=14, pad=20)\n",
        "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, (idx, row) in enumerate(topic_distribution.iterrows()):\n",
        "        ax.text(row['count'] + max(topic_distribution['count']) * 0.01, i,\n",
        "                f\"{int(row['count']):,} ({row['percentage']:.1f}%)\",\n",
        "                va='center', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../outputs/figures/topic_distribution.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "    print(\"✓ Topic distribution saved\")\n",
        "    plt.close()\n",
        "else:\n",
        "    print(\"WARNING: 'search_query' column not found in dataset\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_summary_stats(data, column):\n",
        "    \"\"\"\n",
        "    Calculate appropriate summary statistics based on distribution shape.\n",
        "    \n",
        "    CRITICAL: Converts data to numeric first.\n",
        "    \"\"\"\n",
        "    # Convert to numeric and drop NaN\n",
        "    numeric_data = pd.to_numeric(data[column], errors='coerce').dropna()\n",
        "    \n",
        "    if len(numeric_data) == 0:\n",
        "        return None\n",
        "    \n",
        "    stats_dict = {\n",
        "        'column': column,\n",
        "        'count': len(numeric_data),\n",
        "        'mean': numeric_data.mean(),\n",
        "        'median': numeric_data.median(),\n",
        "        'std': numeric_data.std(),\n",
        "        'min': numeric_data.min(),\n",
        "        'max': numeric_data.max(),\n",
        "        'Q1': numeric_data.quantile(0.25),\n",
        "        'Q3': numeric_data.quantile(0.75),\n",
        "        'IQR': numeric_data.quantile(0.75) - numeric_data.quantile(0.25),\n",
        "        'skewness': numeric_data.skew(),\n",
        "        'kurtosis': numeric_data.kurtosis()\n",
        "    }\n",
        "    \n",
        "    # Determine if distribution is symmetrical\n",
        "    is_symmetrical = abs(stats_dict['skewness']) < 0.5 and stats_dict['count'] > 30\n",
        "    \n",
        "    if is_symmetrical:\n",
        "        stats_dict['summary'] = f\"{stats_dict['mean']:.2f} ± {stats_dict['std']:.2f} (mean ± SD)\"\n",
        "        stats_dict['stat_type'] = 'mean_sd'\n",
        "    else:\n",
        "        stats_dict['summary'] = f\"{stats_dict['median']:.2f} [{stats_dict['Q1']:.2f}, {stats_dict['Q3']:.2f}] (median [IQR])\"\n",
        "        stats_dict['stat_type'] = 'median_iqr'\n",
        "    \n",
        "    return stats_dict\n",
        "\n",
        "# Calculate for all numerical columns\n",
        "print(\"=\" * 60)\n",
        "print(\"Summary Statistics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "summary_results = []\n",
        "for col in numeric_cols:\n",
        "    result = calculate_summary_stats(df, col)\n",
        "    if result:\n",
        "        summary_results.append(result)\n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  {result['summary']}\")\n",
        "        print(f\"  Skewness: {result['skewness']:.3f}, Kurtosis: {result['kurtosis']:.3f}\")\n",
        "        print(f\"  Range: [{result['min']:.2f}, {result['max']:.2f}]\")\n",
        "\n",
        "summary_df = pd.DataFrame(summary_results)\n",
        "summary_df.to_csv('../outputs/tables/summary_statistics.csv', index=False)\n",
        "print(\"\\n✓ Summary statistics saved\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"NOTEBOOK 2 COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Next step: Run Notebook 3 (Preprocessing and Feature Engineering)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
